{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e173663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import re              # Used to search for a string in a line\n",
    "import numpy as np\n",
    "import urllib.request  # Used to download the file\n",
    "import requests        # Used to check if a URL exists\n",
    "import warnings\n",
    "\n",
    "import pandas as pd    # Just used for the date_range function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "503cebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_HRRR_subset(url, searchString, SAVEDIR='./', dryrun=False):\n",
    "    \"\"\"\n",
    "    Download a subset of GRIB fields from a HRRR file.\n",
    "    \n",
    "    This assumes there is an index (.idx) file available for the file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "        The URL for the HRRR file you are trying to download. There must be an \n",
    "        index file for the GRIB2 file. For example, if \n",
    "        ``url='https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200624/hrrr.t01z.wrfsfcf17.grib2'``,\n",
    "        then ``https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200624/hrrr.t01z.wrfsfcf17.grib2.idx``\n",
    "        must also exist on the server.\n",
    "    searchString : str\n",
    "        The string you are looking for in each line of the index file. \n",
    "        Take a look at the \n",
    "        .idx file at https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200624/hrrr.t01z.wrfsfcf17.grib2.idx\n",
    "        to get familiar with what is in each line.\n",
    "        Also look at this webpage: http://hrrr.chpc.utah.edu/HRRR_archive/hrrr_sfc_table_f00-f01.html\n",
    "        for additional details.**You should focus on the variable and level \n",
    "        field for your searches**.\n",
    "        \n",
    "        You may use regular expression syntax to customize your search. \n",
    "        Check out this regulare expression cheatsheet:\n",
    "        https://link.medium.com/7rxduD2e06\n",
    "        \n",
    "        Here are a few examples that can help you get started\n",
    "        \n",
    "        ================ ===============================================\n",
    "        ``searchString`` Messages that will be downloaded\n",
    "        ================ ===============================================\n",
    "        ':TMP:2 m'       Temperature at 2 m.\n",
    "        ':TMP:'          Temperature fields at all levels.\n",
    "        ':500 mb:'       All variables on the 500 mb level.\n",
    "        ':APCP:'         All accumulated precipitation fields.\n",
    "        ':UGRD:10 m:'    U wind component at 10 meters.\n",
    "        ':(U|V)GRD:'     U and V wind component at all levels.\n",
    "        ':.GRD:'         (Same as above)\n",
    "        ':(TMP|DPT):'    Temperature and Dew Point for all levels .\n",
    "        ':(TMP|DPT|RH):' TMP, DPT, and Relative Humidity for all levels.\n",
    "        ':REFC:'         Composite Reflectivity\n",
    "        ':surface:'      All variables at the surface.\n",
    "        ================ ===============================================    \n",
    "        \n",
    "    SAVEDIR : string\n",
    "        Directory path to save the file, default is the current directory.\n",
    "    dryrun : bool\n",
    "        If True, do not actually download, but print out what the function will\n",
    "        attempt to do.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    The path and name of the new file.\n",
    "    \"\"\"\n",
    "    # Ping Pando first. This *might* prevent a \"bad handshake\" error.\n",
    "    if 'pando' in url:\n",
    "        try:\n",
    "            requests.head('https://pando-rgw01.chpc.utah.edu/')\n",
    "        except:\n",
    "            print('bad handshake...am I able to on?')\n",
    "            pass\n",
    "    \n",
    "    # Make SAVEDIR if path doesn't exist\n",
    "    if not os.path.exists(SAVEDIR):\n",
    "        os.makedirs(SAVEDIR)\n",
    "        print(f'Created directory: {SAVEDIR}')\n",
    "\n",
    "    \n",
    "    # Make a request for the .idx file for the above URL\n",
    "    idx = url + '.idx'\n",
    "    r = requests.get(idx)\n",
    "\n",
    "    # Check that the file exists. If there isn't an index, you will get a 404 error.\n",
    "    if not r.ok: \n",
    "        print('‚ùå SORRY! Status Code:', r.status_code, r.reason)\n",
    "        print(f'‚ùå It does not look like the index file exists: {idx}')\n",
    "\n",
    "    # Read the text lines of the request\n",
    "    lines = r.text.split('\\n')\n",
    "    \n",
    "    # Search expression\n",
    "    expr = re.compile(searchString)\n",
    "\n",
    "    # Store the byte ranges in a dictionary\n",
    "    #     {byte-range-as-string: line}\n",
    "    byte_ranges = {}\n",
    "    for n, line in enumerate(lines, start=1):\n",
    "        # n is the line number (starting from 1) so that when we call for \n",
    "        # `lines[n]` it will give us the next line. (Clear as mud??)\n",
    "\n",
    "        # Use the compiled regular expression to search the line\n",
    "        if expr.search(line):   \n",
    "            # aka, if the line contains the string we are looking for...\n",
    "\n",
    "            # Get the beginning byte in the line we found\n",
    "            parts = line.split(':')\n",
    "            rangestart = int(parts[1])\n",
    "\n",
    "            # Get the beginning byte in the next line...\n",
    "            if n+1 < len(lines):\n",
    "                # ...if there is a next line\n",
    "                parts = lines[n].split(':')\n",
    "                rangeend = int(parts[1])\n",
    "            else:\n",
    "                # ...if there isn't a next line, then go to the end of the file.\n",
    "                rangeend = ''\n",
    "\n",
    "            # Store the byte-range string in our dictionary, \n",
    "            # and keep the line information too so we can refer back to it.\n",
    "            byte_ranges[f'{rangestart}-{rangeend}'] = line\n",
    "    \n",
    "    # What should we name the file we save this data to?\n",
    "    # Let's name it something like `subset_20200624_hrrr.t01z.wrfsfcf17.grib2`\n",
    "    runDate = list(byte_ranges.items())[0][1].split(':')[2][2:-2]\n",
    "    outFile = '_'.join(['subset', runDate, url.split('/')[-1]])\n",
    "    outFile = os.path.join(SAVEDIR, outFile)\n",
    "    \n",
    "    for i, (byteRange, line) in enumerate(byte_ranges.items()):\n",
    "        \n",
    "        if i == 0:\n",
    "            # If we are working on the first item, overwrite the existing file.\n",
    "            curl = f'curl -s --range {byteRange} {url} > {outFile}'\n",
    "        else:\n",
    "            # If we are working on not the first item, append the existing file.\n",
    "            curl = f'curl -s --range {byteRange} {url} >> {outFile}'\n",
    "            \n",
    "        num, byte, date, var, level, forecast, _ = line.split(':')\n",
    "        \n",
    "        if dryrun:\n",
    "            print(f'  üåµ Dry Run: Found GRIB line [{num}]: variable={var}, level={level}, forecast={forecast}')\n",
    "            print(f'  üåµ Dry Run: `{curl}`')\n",
    "        else:\n",
    "            print(f'  Downloading GRIB line [{num}]: variable={var}, level={level}, forecast={forecast}')    \n",
    "            os.system(curl)\n",
    "    \n",
    "    if dryrun:\n",
    "        print(f'üåµ Dry Run: Success! Searched for [{searchString}] and found [{len(byte_ranges)}] GRIB fields.')\n",
    "        print(f'üåµ Dry Run: Would save as {outFile}')\n",
    "    else:\n",
    "        print(f'‚úÖ Success! Searched for [{searchString}] and got [{len(byte_ranges)}] GRIB fields.')\n",
    "        print(f'    Saved as {outFile}')\n",
    "    \n",
    "        return outFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8cd7081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2020-03-27 18:00:00'], dtype='datetime64[ns]', freq='H')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the start and end date for the HRRR files we want to download\n",
    "sDATE = datetime(2020,3,27,18)\n",
    "eDATE = datetime(2020,3,27,18)\n",
    "\n",
    "# Create a list of datetimes we want to download with Pandas `date_range` function.\n",
    "# The HRRR model is run every hour, so make a list of every hour\n",
    "DATES = pd.date_range(sDATE, eDATE, freq='1H')\n",
    "DATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9736d53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fxx = range(0,37,1)\n",
    "list(fxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "769c4014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf00.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf01.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf02.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf03.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf04.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf05.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf06.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf07.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf08.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf09.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf10.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf11.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf12.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf13.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf14.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf15.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf16.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf17.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf18.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf19.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf20.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf21.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf22.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf23.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf24.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf25.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf26.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf27.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf28.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf29.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf30.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf31.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf32.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf33.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf34.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf35.grib2',\n",
       " 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200327/hrrr.t18z.wrfsfcf36.grib2']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL_list = [f'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/{DATE:%Y%m%d}/hrrr.t{DATE:%H}z.wrfsfcf{f:02d}.grib2' for DATE in DATES for f in fxx]\n",
    "URL_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f2f3181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading GRIB line [66]: variable=TMP, level=2 m above ground, forecast=anl\n",
      "‚úÖ Success! Searched for [:TMP:2 m] and got [1] GRIB fields.\n",
      "    Saved as ./putInThisDir/subset_20200327_hrrr.t18z.wrfsfcf00.grib2\n",
      "  Downloading GRIB line [66]: variable=TMP, level=2 m above ground, forecast=1 hour fcst\n",
      "‚úÖ Success! Searched for [:TMP:2 m] and got [1] GRIB fields.\n",
      "    Saved as ./putInThisDir/subset_20200327_hrrr.t18z.wrfsfcf01.grib2\n",
      "  Downloading GRIB line [66]: variable=TMP, level=2 m above ground, forecast=2 hour fcst\n",
      "‚úÖ Success! Searched for [:TMP:2 m] and got [1] GRIB fields.\n",
      "    Saved as ./putInThisDir/subset_20200327_hrrr.t18z.wrfsfcf02.grib2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 7, got 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-582cb9312553>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcheck_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Content-Length'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1000000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcheck_exists\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcheck_content\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mdownload_HRRR_subset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mURL_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\":TMP:2 m\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSAVEDIR\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./putInThisDir/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-510daccdb685>\u001b[0m in \u001b[0;36mdownload_HRRR_subset\u001b[1;34m(url, searchString, SAVEDIR, dryrun)\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mcurl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'curl -s --range {byteRange} {url} >> {outFile}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbyte\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforecast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdryrun\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 7, got 6)"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(URL_list)-1):\n",
    "    head = requests.head(URL_list[i])\n",
    "    check_exists = head.ok\n",
    "    check_content = int(head.raw.info()['Content-Length']) > 1000000\n",
    "    if check_exists and check_content:\n",
    "        download_HRRR_subset(URL_list[i],\":TMP:2 m\",SAVEDIR='./putInThisDir/')\n",
    "    else:\n",
    "        print()\n",
    "        print(f'‚ùå WARNING: Status code {head.status_code}: {head.reason}. Content-Length: {int(head.raw.info()[\"Content-Length\"]):,} bytes')\n",
    "        print(f'‚ùå Could not download {head.url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b0953b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e7e885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
