{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c270948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import re              # Used to search for a string in a line\n",
    "import numpy as np\n",
    "import urllib.request  # Used to download the file\n",
    "import requests        # Used to check if a URL exists\n",
    "import warnings\n",
    "import pandas as pd    # Just used for the date_range function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340cb444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_subHRRR_subset(url, searchString, SAVEDIR='./', dryrun=False):\n",
    "    \"\"\"\n",
    "    Download a subset of GRIB fields from a HRRR file.\n",
    "    \n",
    "    This assumes there is an index (.idx) file available for the file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "        The URL for the HRRR file you are trying to download. There must be an \n",
    "        index file for the GRIB2 file. For example, if \n",
    "        ``url='https://noaa-hrrr-bdp-pds.s3.amazonaws.com/hrrr.20200101/conus/hrrr.t00z.wrfsubhf00.grib2'``,\n",
    "        then ``https://noaa-hrrr-bdp-pds.s3.amazonaws.com/hrrr.20200101/conus/hrrr.t00z.wrfsubhf00.grib2.idx``\n",
    "        must also exist on the server.\n",
    "    searchString : str\n",
    "        The string you are looking for in each line of the index file. \n",
    "        Take a look at the \n",
    "        .idx file at https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200624/hrrr.t01z.wrfsfcf17.grib2.idx\n",
    "        to get familiar with what is in each line.\n",
    "        Also look at this webpage: http://hrrr.chpc.utah.edu/HRRR_archive/hrrr_sfc_table_f00-f01.html\n",
    "        for additional details.**You should focus on the variable and level \n",
    "        field for your searches**.\n",
    "        \n",
    "        You may use regular expression syntax to customize your search. \n",
    "        Check out this regulare expression cheatsheet:\n",
    "        https://link.medium.com/7rxduD2e06\n",
    "        \n",
    "        Here are a few examples that can help you get started\n",
    "        \n",
    "        ================ ===============================================\n",
    "        ``searchString`` Messages that will be downloaded\n",
    "        ================ ===============================================\n",
    "        ':TMP:2 m'       Temperature at 2 m.\n",
    "        ':TMP:'          Temperature fields at all levels.\n",
    "        ':500 mb:'       All variables on the 500 mb level.\n",
    "        ':APCP:'         All accumulated precipitation fields.\n",
    "        ':UGRD:10 m:'    U wind component at 10 meters.\n",
    "        ':(U|V)GRD:'     U and V wind component at all levels.\n",
    "        ':.GRD:'         (Same as above)\n",
    "        ':(TMP|DPT):'    Temperature and Dew Point for all levels .\n",
    "        ':(TMP|DPT|RH):' TMP, DPT, and Relative Humidity for all levels.\n",
    "        ':REFC:'         Composite Reflectivity\n",
    "        ':surface:'      All variables at the surface.\n",
    "        ================ ===============================================    \n",
    "        \n",
    "    SAVEDIR : string\n",
    "        Directory path to save the file, default is the current directory.\n",
    "    dryrun : bool\n",
    "        If True, do not actually download, but print out what the function will\n",
    "        attempt to do.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    The path and name of the new file.\n",
    "    \"\"\"\n",
    "    # Ping Pando first. This *might* prevent a \"bad handshake\" error.\n",
    "    if 'amazonaws' in url:\n",
    "        try:\n",
    "            requests.head('https://noaa-hrrr-bdp-pds.s3.amazonaws.com/')\n",
    "        except:\n",
    "            print('bad handshake...am I able to on?')\n",
    "            pass\n",
    "    \n",
    "    # Make SAVEDIR if path doesn't exist\n",
    "    if not os.path.exists(SAVEDIR):\n",
    "        os.makedirs(SAVEDIR)\n",
    "        print(f'Created directory: {SAVEDIR}')\n",
    "\n",
    "    \n",
    "    # Make a request for the .idx file for the above URL\n",
    "    idx = url + '.idx'\n",
    "    r = requests.get(idx)\n",
    "\n",
    "    # Check that the file exists. If there isn't an index, you will get a 404 error.\n",
    "    if not r.ok: \n",
    "        print('‚ùå SORRY! Status Code:', r.status_code, r.reason)\n",
    "        print(f'‚ùå It does not look like the index file exists: {idx}')\n",
    "\n",
    "    # Read the text lines of the request\n",
    "    lines = r.text.split('\\n')\n",
    "    \n",
    "    # Search expression\n",
    "    expr = re.compile(searchString)\n",
    "\n",
    "    # Store the byte ranges in a dictionary\n",
    "    #     {byte-range-as-string: line}\n",
    "    byte_ranges = {}\n",
    "    for n, line in enumerate(lines, start=1):\n",
    "        # n is the line number (starting from 1) so that when we call for \n",
    "        # `lines[n]` it will give us the next line. (Clear as mud??)\n",
    "\n",
    "        # Use the compiled regular expression to search the line\n",
    "        if expr.search(line):   \n",
    "            # aka, if the line contains the string we are looking for...\n",
    "\n",
    "            # Get the beginning byte in the line we found\n",
    "            parts = line.split(':')\n",
    "            rangestart = int(parts[1])\n",
    "\n",
    "            # Get the beginning byte in the next line...\n",
    "            if n+1 < len(lines):\n",
    "                # ...if there is a next line\n",
    "                parts = lines[n].split(':')\n",
    "                rangeend = int(parts[1])\n",
    "            else:\n",
    "                # ...if there isn't a next line, then go to the end of the file.\n",
    "                rangeend = ''\n",
    "\n",
    "            # Store the byte-range string in our dictionary, \n",
    "            # and keep the line information too so we can refer back to it.\n",
    "            byte_ranges[f'{rangestart}-{rangeend}'] = line\n",
    "    \n",
    "    # What should we name the file we save this data to?\n",
    "    # Let's name it something like `subset_20200624_hrrr.t01z.wrfsfcf17.grib2`\n",
    "    runDate = list(byte_ranges.items())[0][1].split(':')[2][2:-2]\n",
    "    outFile = '_'.join(['subset', runDate, url.split('/')[-1]])\n",
    "    outFile = os.path.join(SAVEDIR, outFile)\n",
    "    \n",
    "    for i, (byteRange, line) in enumerate(byte_ranges.items()):\n",
    "        \n",
    "        if i == 0:\n",
    "            # If we are working on the first item, overwrite the existing file.\n",
    "            curl = f'curl -s --range {byteRange} {url} > {outFile}'\n",
    "        else:\n",
    "            # If we are working on not the first item, append the existing file.\n",
    "            curl = f'curl -s --range {byteRange} {url} >> {outFile}'\n",
    "            \n",
    "        num, byte, date, var, level, forecast, _ = line.split(':')\n",
    "        \n",
    "        if dryrun:\n",
    "            print(f'  üåµ Dry Run: Found GRIB line [{num}]: variable={var}, level={level}, forecast={forecast}')\n",
    "            print(f'  üåµ Dry Run: `{curl}`')\n",
    "        else:\n",
    "            print(f'  Downloading GRIB line [{num}]: variable={var}, level={level}, forecast={forecast}')    \n",
    "            os.system(curl)\n",
    "    \n",
    "    if dryrun:\n",
    "        print(f'üåµ Dry Run: Success! Searched for [{searchString}] and found [{len(byte_ranges)}] GRIB fields.')\n",
    "        print(f'üåµ Dry Run: Would save as {outFile}')\n",
    "    else:\n",
    "        print(f'‚úÖ Success! Searched for [{searchString}] and got [{len(byte_ranges)}] GRIB fields.')\n",
    "        print(f'    Saved as {outFile}')\n",
    "    \n",
    "        return outFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74456316",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://noaa-hrrr-bdp-pds.s3.amazonaws.com/hrrr.20200101/conus/hrrr.t00z.wrfsubhf00.grib2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3fa3184d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2020-09-23 16:00:00'], dtype='datetime64[ns]', freq='H')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the start and end date for the HRRR files we want to download\n",
    "sDATE = datetime(2020,9,23,16)\n",
    "eDATE = datetime(2020,9,23,16)\n",
    "\n",
    "# Create a list of datetimes we want to download with Pandas `date_range` function.\n",
    "# The HRRR model is run every hour, so make a list of every hour\n",
    "DATES = pd.date_range(sDATE, eDATE, freq='1H')\n",
    "DATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "70550ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fxx = range(7,19,1)\n",
    "list(fxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0d3b97f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://noaa-hrrr-bdp-pds.s3.amazonaws.com/hrrr.20200923/conus/hrrr.t16z.wrfsubhf07.grib2',\n",
       " 'https://noaa-hrrr-bdp-pds.s3.amazonaws.com/hrrr.20200923/conus/hrrr.t16z.wrfsubhf08.grib2',\n",
       " 'https://noaa-hrrr-bdp-pds.s3.amazonaws.com/hrrr.20200923/conus/hrrr.t16z.wrfsubhf09.grib2',\n",
       " 'https://noaa-hrrr-bdp-pds.s3.amazonaws.com/hrrr.20200923/conus/hrrr.t16z.wrfsubhf10.grib2',\n",
       " 'https://noaa-hrrr-bdp-pds.s3.amazonaws.com/hrrr.20200923/conus/hrrr.t16z.wrfsubhf11.grib2',\n",
       " 'https://noaa-hrrr-bdp-pds.s3.amazonaws.com/hrrr.20200923/conus/hrrr.t16z.wrfsubhf12.grib2',\n",
       " 'https://noaa-hrrr-bdp-pds.s3.amazonaws.com/hrrr.20200923/conus/hrrr.t16z.wrfsubhf13.grib2',\n",
       " 'https://noaa-hrrr-bdp-pds.s3.amazonaws.com/hrrr.20200923/conus/hrrr.t16z.wrfsubhf14.grib2',\n",
       " 'https://noaa-hrrr-bdp-pds.s3.amazonaws.com/hrrr.20200923/conus/hrrr.t16z.wrfsubhf15.grib2',\n",
       " 'https://noaa-hrrr-bdp-pds.s3.amazonaws.com/hrrr.20200923/conus/hrrr.t16z.wrfsubhf16.grib2',\n",
       " 'https://noaa-hrrr-bdp-pds.s3.amazonaws.com/hrrr.20200923/conus/hrrr.t16z.wrfsubhf17.grib2',\n",
       " 'https://noaa-hrrr-bdp-pds.s3.amazonaws.com/hrrr.20200923/conus/hrrr.t16z.wrfsubhf18.grib2']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL_list = [f'https://noaa-hrrr-bdp-pds.s3.amazonaws.com/hrrr.{DATE:%Y%m%d}/conus/hrrr.t{DATE:%H}z.wrfsubhf{f:02d}.grib2' for DATE in DATES for f in fxx]\n",
    "URL_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bed61141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading GRIB line [13]: variable=TMP, level=2 m above ground, forecast=375 min fcst\n",
      "  Downloading GRIB line [56]: variable=TMP, level=2 m above ground, forecast=390 min fcst\n",
      "  Downloading GRIB line [99]: variable=TMP, level=2 m above ground, forecast=405 min fcst\n",
      "  Downloading GRIB line [142]: variable=TMP, level=2 m above ground, forecast=420 min fcst\n",
      "‚úÖ Success! Searched for [:TMP:2 m] and got [4] GRIB fields.\n",
      "    Saved as ./putInThisDir/subset_20200923_hrrr.t16z.wrfsubhf07.grib2\n",
      "  Downloading GRIB line [13]: variable=TMP, level=2 m above ground, forecast=435 min fcst\n",
      "  Downloading GRIB line [56]: variable=TMP, level=2 m above ground, forecast=450 min fcst\n",
      "  Downloading GRIB line [99]: variable=TMP, level=2 m above ground, forecast=465 min fcst\n",
      "  Downloading GRIB line [142]: variable=TMP, level=2 m above ground, forecast=480 min fcst\n",
      "‚úÖ Success! Searched for [:TMP:2 m] and got [4] GRIB fields.\n",
      "    Saved as ./putInThisDir/subset_20200923_hrrr.t16z.wrfsubhf08.grib2\n",
      "  Downloading GRIB line [13]: variable=TMP, level=2 m above ground, forecast=495 min fcst\n",
      "  Downloading GRIB line [56]: variable=TMP, level=2 m above ground, forecast=510 min fcst\n",
      "  Downloading GRIB line [99]: variable=TMP, level=2 m above ground, forecast=525 min fcst\n",
      "  Downloading GRIB line [142]: variable=TMP, level=2 m above ground, forecast=540 min fcst\n",
      "‚úÖ Success! Searched for [:TMP:2 m] and got [4] GRIB fields.\n",
      "    Saved as ./putInThisDir/subset_20200923_hrrr.t16z.wrfsubhf09.grib2\n",
      "  Downloading GRIB line [13]: variable=TMP, level=2 m above ground, forecast=555 min fcst\n",
      "  Downloading GRIB line [56]: variable=TMP, level=2 m above ground, forecast=570 min fcst\n",
      "  Downloading GRIB line [99]: variable=TMP, level=2 m above ground, forecast=585 min fcst\n",
      "  Downloading GRIB line [142]: variable=TMP, level=2 m above ground, forecast=600 min fcst\n",
      "‚úÖ Success! Searched for [:TMP:2 m] and got [4] GRIB fields.\n",
      "    Saved as ./putInThisDir/subset_20200923_hrrr.t16z.wrfsubhf10.grib2\n",
      "  Downloading GRIB line [13]: variable=TMP, level=2 m above ground, forecast=615 min fcst\n",
      "  Downloading GRIB line [56]: variable=TMP, level=2 m above ground, forecast=630 min fcst\n",
      "  Downloading GRIB line [99]: variable=TMP, level=2 m above ground, forecast=645 min fcst\n",
      "  Downloading GRIB line [142]: variable=TMP, level=2 m above ground, forecast=660 min fcst\n",
      "‚úÖ Success! Searched for [:TMP:2 m] and got [4] GRIB fields.\n",
      "    Saved as ./putInThisDir/subset_20200923_hrrr.t16z.wrfsubhf11.grib2\n",
      "  Downloading GRIB line [13]: variable=TMP, level=2 m above ground, forecast=675 min fcst\n",
      "  Downloading GRIB line [56]: variable=TMP, level=2 m above ground, forecast=690 min fcst\n",
      "  Downloading GRIB line [99]: variable=TMP, level=2 m above ground, forecast=705 min fcst\n",
      "  Downloading GRIB line [142]: variable=TMP, level=2 m above ground, forecast=720 min fcst\n",
      "‚úÖ Success! Searched for [:TMP:2 m] and got [4] GRIB fields.\n",
      "    Saved as ./putInThisDir/subset_20200923_hrrr.t16z.wrfsubhf12.grib2\n",
      "  Downloading GRIB line [13]: variable=TMP, level=2 m above ground, forecast=735 min fcst\n",
      "  Downloading GRIB line [56]: variable=TMP, level=2 m above ground, forecast=750 min fcst\n",
      "  Downloading GRIB line [99]: variable=TMP, level=2 m above ground, forecast=765 min fcst\n",
      "  Downloading GRIB line [142]: variable=TMP, level=2 m above ground, forecast=780 min fcst\n",
      "‚úÖ Success! Searched for [:TMP:2 m] and got [4] GRIB fields.\n",
      "    Saved as ./putInThisDir/subset_20200923_hrrr.t16z.wrfsubhf13.grib2\n",
      "  Downloading GRIB line [13]: variable=TMP, level=2 m above ground, forecast=795 min fcst\n",
      "  Downloading GRIB line [56]: variable=TMP, level=2 m above ground, forecast=810 min fcst\n",
      "  Downloading GRIB line [99]: variable=TMP, level=2 m above ground, forecast=825 min fcst\n",
      "  Downloading GRIB line [142]: variable=TMP, level=2 m above ground, forecast=840 min fcst\n",
      "‚úÖ Success! Searched for [:TMP:2 m] and got [4] GRIB fields.\n",
      "    Saved as ./putInThisDir/subset_20200923_hrrr.t16z.wrfsubhf14.grib2\n",
      "  Downloading GRIB line [13]: variable=TMP, level=2 m above ground, forecast=855 min fcst\n",
      "  Downloading GRIB line [56]: variable=TMP, level=2 m above ground, forecast=870 min fcst\n",
      "  Downloading GRIB line [99]: variable=TMP, level=2 m above ground, forecast=885 min fcst\n",
      "  Downloading GRIB line [142]: variable=TMP, level=2 m above ground, forecast=900 min fcst\n",
      "‚úÖ Success! Searched for [:TMP:2 m] and got [4] GRIB fields.\n",
      "    Saved as ./putInThisDir/subset_20200923_hrrr.t16z.wrfsubhf15.grib2\n",
      "  Downloading GRIB line [13]: variable=TMP, level=2 m above ground, forecast=915 min fcst\n",
      "  Downloading GRIB line [56]: variable=TMP, level=2 m above ground, forecast=930 min fcst\n",
      "  Downloading GRIB line [99]: variable=TMP, level=2 m above ground, forecast=945 min fcst\n",
      "  Downloading GRIB line [142]: variable=TMP, level=2 m above ground, forecast=960 min fcst\n",
      "‚úÖ Success! Searched for [:TMP:2 m] and got [4] GRIB fields.\n",
      "    Saved as ./putInThisDir/subset_20200923_hrrr.t16z.wrfsubhf16.grib2\n",
      "  Downloading GRIB line [13]: variable=TMP, level=2 m above ground, forecast=975 min fcst\n",
      "  Downloading GRIB line [56]: variable=TMP, level=2 m above ground, forecast=990 min fcst\n",
      "  Downloading GRIB line [99]: variable=TMP, level=2 m above ground, forecast=1005 min fcst\n",
      "  Downloading GRIB line [142]: variable=TMP, level=2 m above ground, forecast=1020 min fcst\n",
      "‚úÖ Success! Searched for [:TMP:2 m] and got [4] GRIB fields.\n",
      "    Saved as ./putInThisDir/subset_20200923_hrrr.t16z.wrfsubhf17.grib2\n",
      "  Downloading GRIB line [13]: variable=TMP, level=2 m above ground, forecast=1035 min fcst\n",
      "  Downloading GRIB line [56]: variable=TMP, level=2 m above ground, forecast=1050 min fcst\n",
      "  Downloading GRIB line [99]: variable=TMP, level=2 m above ground, forecast=1065 min fcst\n",
      "  Downloading GRIB line [142]: variable=TMP, level=2 m above ground, forecast=1080 min fcst\n",
      "‚úÖ Success! Searched for [:TMP:2 m] and got [4] GRIB fields.\n",
      "    Saved as ./putInThisDir/subset_20200923_hrrr.t16z.wrfsubhf18.grib2\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(URL_list)):\n",
    "    head = requests.head(URL_list[i])\n",
    "    check_exists = head.ok\n",
    "    check_content = int(head.raw.info()['Content-Length']) > 1000000\n",
    "    if check_exists and check_content:\n",
    "        download_subHRRR_subset(URL_list[i],\":TMP:2 m\",SAVEDIR='./putInThisDir/')\n",
    "    else:\n",
    "        print()\n",
    "        print(f'‚ùå WARNING: Status code {head.status_code}: {head.reason}. Content-Length: {int(head.raw.info()[\"Content-Length\"]):,} bytes')\n",
    "        print(f'‚ùå Could not download {head.url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e19498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48277eab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
